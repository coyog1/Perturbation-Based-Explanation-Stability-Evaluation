{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8281b949-b0d9-4b27-b042-b495517e4154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib\n",
    "sys.path.append(str(pathlib.Path('ford').resolve()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2a2130aa-730e-42b9-b3e4-1aeed8202ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, BatchNormalization, Activation,\n",
    "    GlobalAveragePooling1D, LSTM, Dropout, Dense, concatenate\n",
    ")\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "48ae456c-77ba-463a-84db-c4a230fb0579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_arff(path):\n",
    "    raw, meta = loadarff(path)\n",
    "    data = np.vstack([raw[col] for col in meta]).T\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "# Paths\n",
    "train_path = './data/FordA/FordA_TRAIN.arff'\n",
    "test_path  = './data/FordA/FordA_TEST.arff'\n",
    "\n",
    "# Load\n",
    "train2d = read_arff(train_path)\n",
    "test2d  = read_arff(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1405779f-6d3a-4b21-b973-ea3087473680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (2880, 500, 1) (2880,) (721, 500, 1) (721,) (1320, 500, 1) (1320,)\n"
     ]
    }
   ],
   "source": [
    "# Combine and stratified split (80/20)\n",
    "X = train2d[:, :-1]\n",
    "y = (train2d[:, -1] == 1).astype(np.int32)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert test labels\n",
    "X_test = test2d[:, :-1]\n",
    "y_test = (test2d[:, -1] == 1).astype(np.int32)\n",
    "\n",
    "# --- 2. Per-sample z-normalization ---\n",
    "def z_normalize(samples):\n",
    "    # samples: (n_samples, seq_len)\n",
    "    mean = samples.mean(axis=1, keepdims=True)\n",
    "    std  = samples.std(axis=1, keepdims=True)\n",
    "    return (samples - mean) / (std + 1e-8)\n",
    "\n",
    "X_train = z_normalize(X_train)\n",
    "X_val   = z_normalize(X_val)\n",
    "X_test  = z_normalize(X_test)\n",
    "\n",
    "# Reshape for Keras: (n_samples, seq_len, channels)\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_val   = X_val[...,   np.newaxis]\n",
    "X_test  = X_test[...,  np.newaxis]\n",
    "\n",
    "print('Shapes:', X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c8ff5333-cdb2-4106-9418-e50cb15e876a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 500, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 500, 128)     1152        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 500, 128)     512         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 500, 128)     0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 500, 256)     164096      activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 500, 256)     1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 500, 256)     0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 500, 128)     98432       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 500, 128)     512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 500, 128)     0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 128)          66560       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            257         concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 332,545\n",
      "Trainable params: 331,521\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#--- 3. Model Definition (LSTM-FCN hybrid) ---\n",
    "def build_lstm_fcn(input_shape):\n",
    "    inp = Input(shape=input_shape)\n",
    "    # --- FCN branch ---\n",
    "    x = Conv1D(128, kernel_size=8, padding='same')(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv1D(256, kernel_size=5, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv1D(128, kernel_size=3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # --- LSTM branch ---\n",
    "    y = LSTM(128, dropout=0.8, recurrent_dropout=0.8)(inp)\n",
    "\n",
    "    # --- Merge & Prediction ---\n",
    "    merged = concatenate([x, y])\n",
    "    out = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    return model\n",
    "\n",
    "model = build_lstm_fcn(input_shape=(500,1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "066abd81-32d2-44f9-b94f-31b98f7e6aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Training Setup ---\n",
    "# Directories\n",
    "os.makedirs('models', exist_ok=True)\n",
    "log_dir = os.path.join('logs', 'fit_' + pd.Timestamp.now().strftime('%Y%m%d-%H%M%S'))\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        'models/model_forda_lstm_fcn.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=50,\n",
    "        min_lr=1e-5,\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=100,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    TensorBoard(log_dir=log_dir)\n",
    "]\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3b507c58-b627-46d3-832f-a2c2613d45f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "23/23 [==============================] - 166s 7s/step - loss: 0.2221 - accuracy: 0.9111 - val_loss: 0.7990 - val_accuracy: 0.4480\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.51595\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 147s 6s/step - loss: 0.2446 - accuracy: 0.9017 - val_loss: 0.9979 - val_accuracy: 0.6061\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.51595 to 0.60610, saving model to models\\model_forda_lstm_fcn.h5\n",
      "Epoch 3/300\n",
      "23/23 [==============================] - 140s 6s/step - loss: 0.2451 - accuracy: 0.8979 - val_loss: 0.8130 - val_accuracy: 0.5756\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.60610\n",
      "Epoch 4/300\n",
      "23/23 [==============================] - 140s 6s/step - loss: 0.2400 - accuracy: 0.9021 - val_loss: 0.8615 - val_accuracy: 0.5728\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.60610\n",
      "Epoch 5/300\n",
      "23/23 [==============================] - 139s 6s/step - loss: 0.2481 - accuracy: 0.8962 - val_loss: 0.6175 - val_accuracy: 0.6644\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.60610 to 0.66436, saving model to models\\model_forda_lstm_fcn.h5\n",
      "Epoch 6/300\n",
      "23/23 [==============================] - 147s 6s/step - loss: 0.2237 - accuracy: 0.9083 - val_loss: 0.7490 - val_accuracy: 0.5950\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.66436\n",
      "Epoch 7/300\n",
      "23/23 [==============================] - 143s 6s/step - loss: 0.2245 - accuracy: 0.9069 - val_loss: 0.5478 - val_accuracy: 0.7074\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.66436 to 0.70735, saving model to models\\model_forda_lstm_fcn.h5\n",
      "Epoch 8/300\n",
      "23/23 [==============================] - 139s 6s/step - loss: 0.2105 - accuracy: 0.9132 - val_loss: 0.7039 - val_accuracy: 0.6546\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.70735\n",
      "Epoch 9/300\n",
      "23/23 [==============================] - 140s 6s/step - loss: 0.2195 - accuracy: 0.9115 - val_loss: 0.7213 - val_accuracy: 0.6533\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.70735\n",
      "Epoch 10/300\n",
      "23/23 [==============================] - 140s 6s/step - loss: 0.2271 - accuracy: 0.9049 - val_loss: 0.5050 - val_accuracy: 0.7448\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.70735 to 0.74480, saving model to models\\model_forda_lstm_fcn.h5\n",
      "Epoch 11/300\n",
      "23/23 [==============================] - 140s 6s/step - loss: 0.2128 - accuracy: 0.9132 - val_loss: 0.8665 - val_accuracy: 0.6325\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.74480\n",
      "Epoch 12/300\n",
      "23/23 [==============================] - 139s 6s/step - loss: 0.2128 - accuracy: 0.9128 - val_loss: 0.5736 - val_accuracy: 0.7198\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.74480\n",
      "Epoch 13/300\n",
      "23/23 [==============================] - 141s 6s/step - loss: 0.2124 - accuracy: 0.9177 - val_loss: 0.3607 - val_accuracy: 0.8336\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.74480 to 0.83356, saving model to models\\model_forda_lstm_fcn.h5\n",
      "Epoch 14/300\n",
      "23/23 [==============================] - 140s 6s/step - loss: 0.2029 - accuracy: 0.9215 - val_loss: 0.5456 - val_accuracy: 0.7212\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.83356\n",
      "Epoch 15/300\n",
      "23/23 [==============================] - 139s 6s/step - loss: 0.2242 - accuracy: 0.9076 - val_loss: 0.3328 - val_accuracy: 0.8530\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.83356 to 0.85298, saving model to models\\model_forda_lstm_fcn.h5\n",
      "Epoch 16/300\n",
      "23/23 [==============================] - 138s 6s/step - loss: 0.2321 - accuracy: 0.9056 - val_loss: 1.0571 - val_accuracy: 0.6214\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.85298\n",
      "Epoch 17/300\n",
      "23/23 [==============================] - 140s 6s/step - loss: 0.2171 - accuracy: 0.9135 - val_loss: 0.3473 - val_accuracy: 0.8363\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.85298\n",
      "Epoch 18/300\n",
      "23/23 [==============================] - 140s 6s/step - loss: 0.2047 - accuracy: 0.9236 - val_loss: 0.2606 - val_accuracy: 0.8877\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.85298 to 0.88766, saving model to models\\model_forda_lstm_fcn.h5\n",
      "Epoch 19/300\n",
      "23/23 [==============================] - 142s 6s/step - loss: 0.2132 - accuracy: 0.9149 - val_loss: 0.2729 - val_accuracy: 0.8766\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.88766\n",
      "Epoch 20/300\n",
      "23/23 [==============================] - 140s 6s/step - loss: 0.2087 - accuracy: 0.9181 - val_loss: 1.2277 - val_accuracy: 0.6158\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.88766\n",
      "Epoch 21/300\n",
      "23/23 [==============================] - 149s 6s/step - loss: 0.2210 - accuracy: 0.9076 - val_loss: 0.2233 - val_accuracy: 0.9043\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.88766 to 0.90430, saving model to models\\model_forda_lstm_fcn.h5\n",
      "Epoch 22/300\n",
      "23/23 [==============================] - 139s 6s/step - loss: 0.2102 - accuracy: 0.9142 - val_loss: 0.2866 - val_accuracy: 0.8738\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.90430\n",
      "Epoch 23/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.2010 - accuracy: 0.9174 - val_loss: 0.2175 - val_accuracy: 0.9140\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.90430 to 0.91401, saving model to models\\model_forda_lstm_fcn.h5\n",
      "Epoch 24/300\n",
      "23/23 [==============================] - 139s 6s/step - loss: 0.2010 - accuracy: 0.9191 - val_loss: 0.3028 - val_accuracy: 0.8599\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.91401\n",
      "Epoch 25/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.2023 - accuracy: 0.9205 - val_loss: 0.2207 - val_accuracy: 0.9140\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.91401\n",
      "Epoch 26/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1978 - accuracy: 0.9198 - val_loss: 0.2463 - val_accuracy: 0.8877\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.91401\n",
      "Epoch 27/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1988 - accuracy: 0.9184 - val_loss: 0.3938 - val_accuracy: 0.8280\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.91401\n",
      "Epoch 28/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1980 - accuracy: 0.9184 - val_loss: 0.2427 - val_accuracy: 0.8918\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.91401\n",
      "Epoch 29/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.2016 - accuracy: 0.9163 - val_loss: 0.3455 - val_accuracy: 0.8405\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.91401\n",
      "Epoch 30/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.2000 - accuracy: 0.9208 - val_loss: 0.2530 - val_accuracy: 0.8863\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.91401\n",
      "Epoch 31/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1899 - accuracy: 0.9229 - val_loss: 0.2512 - val_accuracy: 0.8932\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.91401\n",
      "Epoch 32/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1875 - accuracy: 0.9250 - val_loss: 0.2658 - val_accuracy: 0.8821\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.91401\n",
      "Epoch 33/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.2026 - accuracy: 0.9170 - val_loss: 1.3508 - val_accuracy: 0.5950\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.91401\n",
      "Epoch 34/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.2020 - accuracy: 0.9184 - val_loss: 0.2022 - val_accuracy: 0.9182\n",
      "\n",
      "Epoch 00034: val_accuracy improved from 0.91401 to 0.91817, saving model to models\\model_forda_lstm_fcn.h5\n",
      "Epoch 35/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1940 - accuracy: 0.9226 - val_loss: 0.2810 - val_accuracy: 0.8752\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.91817\n",
      "Epoch 36/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1961 - accuracy: 0.9160 - val_loss: 0.2110 - val_accuracy: 0.9098\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.91817\n",
      "Epoch 37/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1995 - accuracy: 0.9181 - val_loss: 0.2155 - val_accuracy: 0.9071\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.91817\n",
      "Epoch 38/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1928 - accuracy: 0.9205 - val_loss: 0.4047 - val_accuracy: 0.8225\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.91817\n",
      "Epoch 39/300\n",
      "23/23 [==============================] - 135s 6s/step - loss: 0.1897 - accuracy: 0.9226 - val_loss: 0.3330 - val_accuracy: 0.8544\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.91817\n",
      "Epoch 40/300\n",
      "23/23 [==============================] - 130s 6s/step - loss: 0.1803 - accuracy: 0.9281 - val_loss: 0.2061 - val_accuracy: 0.9085\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.91817\n",
      "Epoch 41/300\n",
      "23/23 [==============================] - 131s 6s/step - loss: 0.1845 - accuracy: 0.9260 - val_loss: 0.3245 - val_accuracy: 0.8682\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.91817\n",
      "Epoch 42/300\n",
      "23/23 [==============================] - 131s 6s/step - loss: 0.1889 - accuracy: 0.9177 - val_loss: 0.3150 - val_accuracy: 0.8752\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.91817\n",
      "Epoch 43/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.2000 - accuracy: 0.9167 - val_loss: 0.2387 - val_accuracy: 0.8863\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.91817\n",
      "Epoch 44/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1851 - accuracy: 0.9274 - val_loss: 0.7432 - val_accuracy: 0.7268\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.91817\n",
      "Epoch 45/300\n",
      "23/23 [==============================] - 131s 6s/step - loss: 0.1836 - accuracy: 0.9271 - val_loss: 0.3186 - val_accuracy: 0.8558\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.91817\n",
      "Epoch 46/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1854 - accuracy: 0.9260 - val_loss: 0.2211 - val_accuracy: 0.9098\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.91817\n",
      "Epoch 47/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1926 - accuracy: 0.9219 - val_loss: 0.2491 - val_accuracy: 0.8849\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.91817\n",
      "Epoch 48/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1820 - accuracy: 0.9243 - val_loss: 0.3994 - val_accuracy: 0.8308\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.91817\n",
      "Epoch 49/300\n",
      "23/23 [==============================] - 131s 6s/step - loss: 0.1773 - accuracy: 0.9309 - val_loss: 0.2838 - val_accuracy: 0.8821\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.91817\n",
      "Epoch 50/300\n",
      "23/23 [==============================] - 131s 6s/step - loss: 0.1874 - accuracy: 0.9212 - val_loss: 0.2957 - val_accuracy: 0.8682\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.91817\n",
      "Epoch 51/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1952 - accuracy: 0.9184 - val_loss: 0.2161 - val_accuracy: 0.9126\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.91817\n",
      "Epoch 52/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1800 - accuracy: 0.9250 - val_loss: 0.2045 - val_accuracy: 0.9098\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.91817\n",
      "Epoch 53/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1799 - accuracy: 0.9236 - val_loss: 0.4109 - val_accuracy: 0.8114\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.91817\n",
      "Epoch 54/300\n",
      "23/23 [==============================] - 131s 6s/step - loss: 0.1856 - accuracy: 0.9253 - val_loss: 0.2122 - val_accuracy: 0.9126\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.91817\n",
      "Epoch 55/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1824 - accuracy: 0.9316 - val_loss: 0.2396 - val_accuracy: 0.8932\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.91817\n",
      "Epoch 56/300\n",
      "23/23 [==============================] - 131s 6s/step - loss: 0.1905 - accuracy: 0.9250 - val_loss: 0.2902 - val_accuracy: 0.8752\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.91817\n",
      "Epoch 57/300\n",
      "23/23 [==============================] - 131s 6s/step - loss: 0.1789 - accuracy: 0.9271 - val_loss: 0.2393 - val_accuracy: 0.8988\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.91817\n",
      "Epoch 58/300\n",
      "23/23 [==============================] - 131s 6s/step - loss: 0.1759 - accuracy: 0.9285 - val_loss: 0.2360 - val_accuracy: 0.8960\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.91817\n",
      "Epoch 59/300\n",
      "23/23 [==============================] - 131s 6s/step - loss: 0.1890 - accuracy: 0.9212 - val_loss: 0.2181 - val_accuracy: 0.9071\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.91817\n",
      "Epoch 60/300\n",
      "23/23 [==============================] - 131s 6s/step - loss: 0.1836 - accuracy: 0.9278 - val_loss: 0.3557 - val_accuracy: 0.8474\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.91817\n",
      "Epoch 61/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1791 - accuracy: 0.9316 - val_loss: 0.2060 - val_accuracy: 0.9112\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.91817\n",
      "Epoch 62/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1759 - accuracy: 0.9292 - val_loss: 0.2220 - val_accuracy: 0.9015\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.91817\n",
      "Epoch 63/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1853 - accuracy: 0.9198 - val_loss: 0.2141 - val_accuracy: 0.9071\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.91817\n",
      "Epoch 64/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1904 - accuracy: 0.9187 - val_loss: 0.2701 - val_accuracy: 0.8821\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.91817\n",
      "Epoch 65/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1872 - accuracy: 0.9243 - val_loss: 0.2231 - val_accuracy: 0.9029\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.91817\n",
      "Epoch 66/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1883 - accuracy: 0.9215 - val_loss: 0.2422 - val_accuracy: 0.8932\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.91817\n",
      "Epoch 67/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1786 - accuracy: 0.9253 - val_loss: 0.4693 - val_accuracy: 0.8058\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.91817\n",
      "Epoch 68/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1843 - accuracy: 0.9194 - val_loss: 0.3197 - val_accuracy: 0.8474\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.91817\n",
      "Epoch 69/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1825 - accuracy: 0.9243 - val_loss: 0.2033 - val_accuracy: 0.9112\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.91817\n",
      "Epoch 70/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1833 - accuracy: 0.9243 - val_loss: 0.2600 - val_accuracy: 0.8779\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.91817\n",
      "Epoch 71/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1825 - accuracy: 0.9274 - val_loss: 0.2575 - val_accuracy: 0.8960\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.91817\n",
      "Epoch 72/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1760 - accuracy: 0.9306 - val_loss: 0.2072 - val_accuracy: 0.9251\n",
      "\n",
      "Epoch 00072: val_accuracy improved from 0.91817 to 0.92510, saving model to models\\model_forda_lstm_fcn.h5\n",
      "Epoch 73/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1788 - accuracy: 0.9295 - val_loss: 0.2496 - val_accuracy: 0.8849\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.92510\n",
      "Epoch 74/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1785 - accuracy: 0.9281 - val_loss: 0.2174 - val_accuracy: 0.9126\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.92510\n",
      "Epoch 75/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1813 - accuracy: 0.9299 - val_loss: 0.2820 - val_accuracy: 0.8724\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.92510\n",
      "Epoch 76/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1700 - accuracy: 0.9330 - val_loss: 0.2639 - val_accuracy: 0.8835\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.92510\n",
      "Epoch 77/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1931 - accuracy: 0.9233 - val_loss: 0.2203 - val_accuracy: 0.9182\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.92510\n",
      "Epoch 78/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1720 - accuracy: 0.9316 - val_loss: 0.1986 - val_accuracy: 0.9140\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.92510\n",
      "Epoch 79/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1748 - accuracy: 0.9309 - val_loss: 0.2540 - val_accuracy: 0.8752\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.92510\n",
      "Epoch 80/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1751 - accuracy: 0.9240 - val_loss: 0.2039 - val_accuracy: 0.9140\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.92510\n",
      "Epoch 81/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1708 - accuracy: 0.9288 - val_loss: 0.2137 - val_accuracy: 0.9140\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.92510\n",
      "Epoch 82/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1699 - accuracy: 0.9347 - val_loss: 0.2028 - val_accuracy: 0.9196\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.92510\n",
      "Epoch 83/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1779 - accuracy: 0.9247 - val_loss: 0.2075 - val_accuracy: 0.9085\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.92510\n",
      "Epoch 84/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1684 - accuracy: 0.9368 - val_loss: 0.1997 - val_accuracy: 0.9209\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.92510\n",
      "Epoch 85/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1719 - accuracy: 0.9312 - val_loss: 0.1983 - val_accuracy: 0.9168\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.92510\n",
      "Epoch 86/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1791 - accuracy: 0.9240 - val_loss: 0.2034 - val_accuracy: 0.9154\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.92510\n",
      "Epoch 87/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1806 - accuracy: 0.9250 - val_loss: 0.2006 - val_accuracy: 0.9154\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.92510\n",
      "Epoch 88/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1758 - accuracy: 0.9316 - val_loss: 0.2658 - val_accuracy: 0.8738\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.92510\n",
      "Epoch 89/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1792 - accuracy: 0.9274 - val_loss: 0.3393 - val_accuracy: 0.8558\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.92510\n",
      "Epoch 90/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1741 - accuracy: 0.9319 - val_loss: 0.2047 - val_accuracy: 0.9154\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.92510\n",
      "Epoch 91/300\n",
      "23/23 [==============================] - 135s 6s/step - loss: 0.1700 - accuracy: 0.9299 - val_loss: 0.2674 - val_accuracy: 0.8946\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.92510\n",
      "Epoch 92/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1625 - accuracy: 0.9361 - val_loss: 0.2360 - val_accuracy: 0.8974\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.92510\n",
      "Epoch 93/300\n",
      "23/23 [==============================] - 135s 6s/step - loss: 0.1609 - accuracy: 0.9358 - val_loss: 0.1935 - val_accuracy: 0.9293\n",
      "\n",
      "Epoch 00093: val_accuracy improved from 0.92510 to 0.92926, saving model to models\\model_forda_lstm_fcn.h5\n",
      "Epoch 94/300\n",
      "23/23 [==============================] - 136s 6s/step - loss: 0.1642 - accuracy: 0.9361 - val_loss: 0.2068 - val_accuracy: 0.9126\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.92926\n",
      "Epoch 95/300\n",
      "23/23 [==============================] - 136s 6s/step - loss: 0.1656 - accuracy: 0.9361 - val_loss: 0.1961 - val_accuracy: 0.9182\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.92926\n",
      "Epoch 96/300\n",
      "23/23 [==============================] - 135s 6s/step - loss: 0.1665 - accuracy: 0.9368 - val_loss: 0.2898 - val_accuracy: 0.8627\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.92926\n",
      "Epoch 97/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1908 - accuracy: 0.9212 - val_loss: 0.2469 - val_accuracy: 0.8974\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.92926\n",
      "Epoch 98/300\n",
      "23/23 [==============================] - 135s 6s/step - loss: 0.1742 - accuracy: 0.9274 - val_loss: 0.2979 - val_accuracy: 0.8904\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.92926\n",
      "Epoch 99/300\n",
      "23/23 [==============================] - 135s 6s/step - loss: 0.1737 - accuracy: 0.9271 - val_loss: 0.2096 - val_accuracy: 0.9196\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.92926\n",
      "Epoch 100/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1682 - accuracy: 0.9323 - val_loss: 0.2001 - val_accuracy: 0.9209\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.92926\n",
      "Epoch 101/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1663 - accuracy: 0.9309 - val_loss: 0.2743 - val_accuracy: 0.8752\n",
      "\n",
      "Epoch 00101: val_accuracy did not improve from 0.92926\n",
      "Epoch 102/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1656 - accuracy: 0.9340 - val_loss: 0.2380 - val_accuracy: 0.8946\n",
      "\n",
      "Epoch 00102: val_accuracy did not improve from 0.92926\n",
      "Epoch 103/300\n",
      "23/23 [==============================] - 135s 6s/step - loss: 0.1628 - accuracy: 0.9365 - val_loss: 0.3874 - val_accuracy: 0.8405\n",
      "\n",
      "Epoch 00103: val_accuracy did not improve from 0.92926\n",
      "Epoch 104/300\n",
      "23/23 [==============================] - 135s 6s/step - loss: 0.1743 - accuracy: 0.9309 - val_loss: 0.1975 - val_accuracy: 0.9182\n",
      "\n",
      "Epoch 00104: val_accuracy did not improve from 0.92926\n",
      "Epoch 105/300\n",
      "23/23 [==============================] - 136s 6s/step - loss: 0.1632 - accuracy: 0.9330 - val_loss: 0.2564 - val_accuracy: 0.8946\n",
      "\n",
      "Epoch 00105: val_accuracy did not improve from 0.92926\n",
      "Epoch 106/300\n",
      "23/23 [==============================] - 135s 6s/step - loss: 0.1684 - accuracy: 0.9333 - val_loss: 0.2516 - val_accuracy: 0.8904\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 0.92926\n",
      "Epoch 107/300\n",
      "23/23 [==============================] - 136s 6s/step - loss: 0.1633 - accuracy: 0.9340 - val_loss: 0.1963 - val_accuracy: 0.9237\n",
      "\n",
      "Epoch 00107: val_accuracy did not improve from 0.92926\n",
      "Epoch 108/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1565 - accuracy: 0.9396 - val_loss: 0.1986 - val_accuracy: 0.9182\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 0.92926\n",
      "Epoch 109/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1577 - accuracy: 0.9413 - val_loss: 0.2634 - val_accuracy: 0.8766\n",
      "\n",
      "Epoch 00109: val_accuracy did not improve from 0.92926\n",
      "Epoch 110/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1766 - accuracy: 0.9257 - val_loss: 0.2372 - val_accuracy: 0.9085\n",
      "\n",
      "Epoch 00110: val_accuracy did not improve from 0.92926\n",
      "Epoch 111/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1715 - accuracy: 0.9312 - val_loss: 0.2049 - val_accuracy: 0.9182\n",
      "\n",
      "Epoch 00111: val_accuracy did not improve from 0.92926\n",
      "Epoch 112/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1704 - accuracy: 0.9323 - val_loss: 0.2084 - val_accuracy: 0.9112\n",
      "\n",
      "Epoch 00112: val_accuracy did not improve from 0.92926\n",
      "Epoch 113/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1553 - accuracy: 0.9403 - val_loss: 0.1966 - val_accuracy: 0.9154\n",
      "\n",
      "Epoch 00113: val_accuracy did not improve from 0.92926\n",
      "Epoch 114/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1655 - accuracy: 0.9330 - val_loss: 0.3177 - val_accuracy: 0.8599\n",
      "\n",
      "Epoch 00114: val_accuracy did not improve from 0.92926\n",
      "Epoch 115/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1737 - accuracy: 0.9288 - val_loss: 0.3303 - val_accuracy: 0.8613\n",
      "\n",
      "Epoch 00115: val_accuracy did not improve from 0.92926\n",
      "Epoch 116/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1647 - accuracy: 0.9365 - val_loss: 0.3152 - val_accuracy: 0.8669\n",
      "\n",
      "Epoch 00116: val_accuracy did not improve from 0.92926\n",
      "Epoch 117/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1666 - accuracy: 0.9323 - val_loss: 0.2048 - val_accuracy: 0.9154\n",
      "\n",
      "Epoch 00117: val_accuracy did not improve from 0.92926\n",
      "Epoch 118/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1703 - accuracy: 0.9358 - val_loss: 0.2072 - val_accuracy: 0.9140\n",
      "\n",
      "Epoch 00118: val_accuracy did not improve from 0.92926\n",
      "Epoch 119/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1685 - accuracy: 0.9358 - val_loss: 0.2169 - val_accuracy: 0.9015\n",
      "\n",
      "Epoch 00119: val_accuracy did not improve from 0.92926\n",
      "Epoch 120/300\n",
      "23/23 [==============================] - 135s 6s/step - loss: 0.1663 - accuracy: 0.9340 - val_loss: 0.2095 - val_accuracy: 0.9168\n",
      "\n",
      "Epoch 00120: val_accuracy did not improve from 0.92926\n",
      "Epoch 121/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1607 - accuracy: 0.9354 - val_loss: 0.2334 - val_accuracy: 0.9098\n",
      "\n",
      "Epoch 00121: val_accuracy did not improve from 0.92926\n",
      "Epoch 122/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1601 - accuracy: 0.9365 - val_loss: 0.3250 - val_accuracy: 0.8738\n",
      "\n",
      "Epoch 00122: val_accuracy did not improve from 0.92926\n",
      "Epoch 123/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1549 - accuracy: 0.9399 - val_loss: 0.5079 - val_accuracy: 0.8086\n",
      "\n",
      "Epoch 00123: val_accuracy did not improve from 0.92926\n",
      "Epoch 124/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1584 - accuracy: 0.9347 - val_loss: 0.2066 - val_accuracy: 0.9057\n",
      "\n",
      "Epoch 00124: val_accuracy did not improve from 0.92926\n",
      "Epoch 125/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1651 - accuracy: 0.9347 - val_loss: 0.2252 - val_accuracy: 0.9182\n",
      "\n",
      "Epoch 00125: val_accuracy did not improve from 0.92926\n",
      "Epoch 126/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1639 - accuracy: 0.9337 - val_loss: 0.2627 - val_accuracy: 0.8849\n",
      "\n",
      "Epoch 00126: val_accuracy did not improve from 0.92926\n",
      "Epoch 127/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1671 - accuracy: 0.9292 - val_loss: 0.2108 - val_accuracy: 0.9168\n",
      "\n",
      "Epoch 00127: val_accuracy did not improve from 0.92926\n",
      "Epoch 128/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1687 - accuracy: 0.9309 - val_loss: 0.2255 - val_accuracy: 0.9098\n",
      "\n",
      "Epoch 00128: val_accuracy did not improve from 0.92926\n",
      "Epoch 129/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1615 - accuracy: 0.9382 - val_loss: 0.4066 - val_accuracy: 0.8336\n",
      "\n",
      "Epoch 00129: val_accuracy did not improve from 0.92926\n",
      "Epoch 130/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1686 - accuracy: 0.9309 - val_loss: 0.2025 - val_accuracy: 0.9126\n",
      "\n",
      "Epoch 00130: val_accuracy did not improve from 0.92926\n",
      "Epoch 131/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1639 - accuracy: 0.9344 - val_loss: 0.2086 - val_accuracy: 0.9196\n",
      "\n",
      "Epoch 00131: val_accuracy did not improve from 0.92926\n",
      "Epoch 132/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1576 - accuracy: 0.9399 - val_loss: 0.2077 - val_accuracy: 0.9085\n",
      "\n",
      "Epoch 00132: val_accuracy did not improve from 0.92926\n",
      "Epoch 133/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1584 - accuracy: 0.9399 - val_loss: 0.1919 - val_accuracy: 0.9251\n",
      "\n",
      "Epoch 00133: val_accuracy did not improve from 0.92926\n",
      "Epoch 134/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1540 - accuracy: 0.9375 - val_loss: 0.2544 - val_accuracy: 0.8821\n",
      "\n",
      "Epoch 00134: val_accuracy did not improve from 0.92926\n",
      "Epoch 135/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1631 - accuracy: 0.9295 - val_loss: 0.2352 - val_accuracy: 0.9071\n",
      "\n",
      "Epoch 00135: val_accuracy did not improve from 0.92926\n",
      "Epoch 136/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1588 - accuracy: 0.9399 - val_loss: 0.2174 - val_accuracy: 0.9085\n",
      "\n",
      "Epoch 00136: val_accuracy did not improve from 0.92926\n",
      "Epoch 137/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1473 - accuracy: 0.9490 - val_loss: 0.1933 - val_accuracy: 0.9251\n",
      "\n",
      "Epoch 00137: val_accuracy did not improve from 0.92926\n",
      "Epoch 138/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1497 - accuracy: 0.9410 - val_loss: 0.2100 - val_accuracy: 0.9154\n",
      "\n",
      "Epoch 00138: val_accuracy did not improve from 0.92926\n",
      "Epoch 139/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1505 - accuracy: 0.9448 - val_loss: 0.2759 - val_accuracy: 0.8835\n",
      "\n",
      "Epoch 00139: val_accuracy did not improve from 0.92926\n",
      "Epoch 140/300\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.1510 - accuracy: 0.9399 - val_loss: 0.2640 - val_accuracy: 0.8821\n",
      "\n",
      "Epoch 00140: val_accuracy did not improve from 0.92926\n",
      "Epoch 141/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1566 - accuracy: 0.9438 - val_loss: 0.1941 - val_accuracy: 0.9182\n",
      "\n",
      "Epoch 00141: val_accuracy did not improve from 0.92926\n",
      "Epoch 142/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1613 - accuracy: 0.9351 - val_loss: 0.2877 - val_accuracy: 0.8724\n",
      "\n",
      "Epoch 00142: val_accuracy did not improve from 0.92926\n",
      "Epoch 143/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1584 - accuracy: 0.9396 - val_loss: 0.1866 - val_accuracy: 0.9154\n",
      "\n",
      "Epoch 00143: val_accuracy did not improve from 0.92926\n",
      "Epoch 144/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1567 - accuracy: 0.9368 - val_loss: 0.5801 - val_accuracy: 0.7836\n",
      "\n",
      "Epoch 00144: val_accuracy did not improve from 0.92926\n",
      "Epoch 145/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1525 - accuracy: 0.9427 - val_loss: 0.2142 - val_accuracy: 0.9140\n",
      "\n",
      "Epoch 00145: val_accuracy did not improve from 0.92926\n",
      "Epoch 146/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1479 - accuracy: 0.9427 - val_loss: 0.1924 - val_accuracy: 0.9293\n",
      "\n",
      "Epoch 00146: val_accuracy did not improve from 0.92926\n",
      "Epoch 147/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1533 - accuracy: 0.9420 - val_loss: 0.1903 - val_accuracy: 0.9279\n",
      "\n",
      "Epoch 00147: val_accuracy did not improve from 0.92926\n",
      "Epoch 148/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1514 - accuracy: 0.9424 - val_loss: 0.1971 - val_accuracy: 0.9265\n",
      "\n",
      "Epoch 00148: val_accuracy did not improve from 0.92926\n",
      "Epoch 149/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1508 - accuracy: 0.9375 - val_loss: 0.1892 - val_accuracy: 0.9223\n",
      "\n",
      "Epoch 00149: val_accuracy did not improve from 0.92926\n",
      "Epoch 150/300\n",
      "23/23 [==============================] - 137s 6s/step - loss: 0.1457 - accuracy: 0.9483 - val_loss: 0.2460 - val_accuracy: 0.8904\n",
      "\n",
      "Epoch 00150: val_accuracy did not improve from 0.92926\n",
      "Epoch 151/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1558 - accuracy: 0.9399 - val_loss: 0.2917 - val_accuracy: 0.8682\n",
      "\n",
      "Epoch 00151: val_accuracy did not improve from 0.92926\n",
      "Epoch 152/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1543 - accuracy: 0.9431 - val_loss: 0.3333 - val_accuracy: 0.8530\n",
      "\n",
      "Epoch 00152: val_accuracy did not improve from 0.92926\n",
      "Epoch 153/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1536 - accuracy: 0.9368 - val_loss: 0.2104 - val_accuracy: 0.9265\n",
      "\n",
      "Epoch 00153: val_accuracy did not improve from 0.92926\n",
      "Epoch 154/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1499 - accuracy: 0.9417 - val_loss: 0.2255 - val_accuracy: 0.9029\n",
      "\n",
      "Epoch 00154: val_accuracy did not improve from 0.92926\n",
      "Epoch 155/300\n",
      "23/23 [==============================] - 134s 6s/step - loss: 0.1417 - accuracy: 0.9469 - val_loss: 0.2421 - val_accuracy: 0.8863\n",
      "\n",
      "Epoch 00155: val_accuracy did not improve from 0.92926\n",
      "Epoch 156/300\n",
      "23/23 [==============================] - 133s 6s/step - loss: 0.1524 - accuracy: 0.9396 - val_loss: 0.1886 - val_accuracy: 0.9223\n",
      "\n",
      "Epoch 00156: val_accuracy did not improve from 0.92926\n",
      "Epoch 157/300\n",
      "23/23 [==============================] - 135s 6s/step - loss: 0.1476 - accuracy: 0.9444 - val_loss: 0.2164 - val_accuracy: 0.9085\n",
      "\n",
      "Epoch 00157: val_accuracy did not improve from 0.92926\n",
      "Epoch 158/300\n",
      "23/23 [==============================] - 141s 6s/step - loss: 0.1539 - accuracy: 0.9403 - val_loss: 0.2077 - val_accuracy: 0.9182\n",
      "\n",
      "Epoch 00158: val_accuracy did not improve from 0.92926\n",
      "Epoch 159/300\n",
      "23/23 [==============================] - 139s 6s/step - loss: 0.1468 - accuracy: 0.9417 - val_loss: 0.2404 - val_accuracy: 0.8932\n",
      "\n",
      "Epoch 00159: val_accuracy did not improve from 0.92926\n",
      "Epoch 160/300\n",
      "23/23 [==============================] - 139s 6s/step - loss: 0.1441 - accuracy: 0.9434 - val_loss: 0.2311 - val_accuracy: 0.9029\n",
      "\n",
      "Epoch 00160: val_accuracy did not improve from 0.92926\n",
      "Epoch 161/300\n",
      "23/23 [==============================] - 139s 6s/step - loss: 0.1500 - accuracy: 0.9434 - val_loss: 0.5603 - val_accuracy: 0.7961\n",
      "\n",
      "Epoch 00161: val_accuracy did not improve from 0.92926\n",
      "Epoch 162/300\n",
      "23/23 [==============================] - 139s 6s/step - loss: 0.1569 - accuracy: 0.9410 - val_loss: 0.2013 - val_accuracy: 0.9182\n",
      "\n",
      "Epoch 00162: val_accuracy did not improve from 0.92926\n",
      "Epoch 163/300\n",
      "23/23 [==============================] - 139s 6s/step - loss: 0.1511 - accuracy: 0.9417 - val_loss: 0.2748 - val_accuracy: 0.8877\n",
      "\n",
      "Epoch 00163: val_accuracy did not improve from 0.92926\n",
      "Epoch 164/300\n",
      "23/23 [==============================] - 139s 6s/step - loss: 0.1419 - accuracy: 0.9472 - val_loss: 0.2767 - val_accuracy: 0.8807\n",
      "\n",
      "Epoch 00164: val_accuracy did not improve from 0.92926\n",
      "Epoch 165/300\n",
      "23/23 [==============================] - 140s 6s/step - loss: 0.1451 - accuracy: 0.9458 - val_loss: 0.2311 - val_accuracy: 0.9057\n",
      "\n",
      "Epoch 00165: val_accuracy did not improve from 0.92926\n",
      "Epoch 166/300\n",
      "23/23 [==============================] - 139s 6s/step - loss: 0.1556 - accuracy: 0.9361 - val_loss: 0.2331 - val_accuracy: 0.8974\n",
      "\n",
      "Epoch 00166: val_accuracy did not improve from 0.92926\n",
      "Epoch 167/300\n",
      "23/23 [==============================] - 140s 6s/step - loss: 0.1607 - accuracy: 0.9372 - val_loss: 0.1884 - val_accuracy: 0.9251\n",
      "\n",
      "Epoch 00167: val_accuracy did not improve from 0.92926\n",
      "Epoch 168/300\n",
      "23/23 [==============================] - 138s 6s/step - loss: 0.1412 - accuracy: 0.9483 - val_loss: 0.1888 - val_accuracy: 0.9251\n",
      "\n",
      "Epoch 00168: val_accuracy did not improve from 0.92926\n",
      "Epoch 169/300\n",
      "23/23 [==============================] - 139s 6s/step - loss: 0.1387 - accuracy: 0.9448 - val_loss: 0.2163 - val_accuracy: 0.9237\n",
      "\n",
      "Epoch 00169: val_accuracy did not improve from 0.92926\n",
      "Epoch 170/300\n",
      "23/23 [==============================] - 140s 6s/step - loss: 0.1440 - accuracy: 0.9462 - val_loss: 0.2396 - val_accuracy: 0.8918\n",
      "\n",
      "Epoch 00170: val_accuracy did not improve from 0.92926\n",
      "Epoch 171/300\n",
      "23/23 [==============================] - 139s 6s/step - loss: 0.1461 - accuracy: 0.9444 - val_loss: 0.2160 - val_accuracy: 0.9154\n",
      "\n",
      "Epoch 00171: val_accuracy did not improve from 0.92926\n",
      "Epoch 172/300\n",
      "23/23 [==============================] - 139s 6s/step - loss: 0.1591 - accuracy: 0.9378 - val_loss: 0.2081 - val_accuracy: 0.9071\n",
      "\n",
      "Epoch 00172: val_accuracy did not improve from 0.92926\n",
      "Epoch 173/300\n",
      "23/23 [==============================] - 139s 6s/step - loss: 0.1468 - accuracy: 0.9441 - val_loss: 0.2094 - val_accuracy: 0.9140\n",
      "\n",
      "Epoch 00173: val_accuracy did not improve from 0.92926\n",
      "Epoch 174/300\n",
      "23/23 [==============================] - 141s 6s/step - loss: 0.1377 - accuracy: 0.9503 - val_loss: 0.1989 - val_accuracy: 0.9112\n",
      "\n",
      "Epoch 00174: val_accuracy did not improve from 0.92926\n",
      "Epoch 175/300\n",
      "23/23 [==============================] - 154s 7s/step - loss: 0.1479 - accuracy: 0.9420 - val_loss: 0.2271 - val_accuracy: 0.9057\n",
      "\n",
      "Epoch 00175: val_accuracy did not improve from 0.92926\n",
      "Epoch 176/300\n",
      "23/23 [==============================] - 150s 7s/step - loss: 0.1490 - accuracy: 0.9417 - val_loss: 0.2981 - val_accuracy: 0.8890\n",
      "\n",
      "Epoch 00176: val_accuracy did not improve from 0.92926\n",
      "Epoch 177/300\n",
      "23/23 [==============================] - 146s 6s/step - loss: 0.1699 - accuracy: 0.9281 - val_loss: 0.2953 - val_accuracy: 0.8807\n",
      "\n",
      "Epoch 00177: val_accuracy did not improve from 0.92926\n",
      "Epoch 178/300\n",
      "23/23 [==============================] - 147s 6s/step - loss: 0.1629 - accuracy: 0.9368 - val_loss: 0.3971 - val_accuracy: 0.8419\n",
      "\n",
      "Epoch 00178: val_accuracy did not improve from 0.92926\n",
      "Epoch 179/300\n",
      "23/23 [==============================] - 145s 6s/step - loss: 0.1490 - accuracy: 0.9392 - val_loss: 0.1962 - val_accuracy: 0.9196\n",
      "\n",
      "Epoch 00179: val_accuracy did not improve from 0.92926\n",
      "Epoch 180/300\n",
      " 4/23 [====>.........................] - ETA: 1:53 - loss: 0.1401 - accuracy: 0.9512"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11132/3587685560.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m history = model.fit(\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\UvA\\Courses\\Master's Thesis\\CODE\\venv\\lime_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\UvA\\Courses\\Master's Thesis\\CODE\\venv\\lime_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\UvA\\Courses\\Master's Thesis\\CODE\\venv\\lime_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\UvA\\Courses\\Master's Thesis\\CODE\\venv\\lime_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\UvA\\Courses\\Master's Thesis\\CODE\\venv\\lime_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\OneDrive\\UvA\\Courses\\Master's Thesis\\CODE\\venv\\lime_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\UvA\\Courses\\Master's Thesis\\CODE\\venv\\lime_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=300,\n",
    "    batch_size=128,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print('Training complete, best model saved to models/best_forda_lstm_fcn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cd51cccf-c845-47b5-9701-184dcda52628",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = tf.keras.models.load_model('models/model_forda_lstm_fcn.h5')\n",
    "best.save('models/model_lstm_forda.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5bd56341-bac5-487b-b1f1-0b5beb436520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Load the best model from the specified path\n",
    "model = keras.models.load_model(\"./models/model_lstm_forda.keras\")\n",
    "print(\"Best model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e5ab2f80-e19d-4457-96f3-02918366baa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2017\n",
      "Test Accuracy: 0.9159\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test_s, verbose=0)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8844c7e9-3ba7-4696-a33f-5afd77fb3b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2017\n",
      "Test Accuracy: 0.9159\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "09fd9e6d-3d2d-4f67-87b1-1480954e1cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def draw_roc_keras(model, x, y_true):\n",
    "    # Get predicted probabilities for the positive class\n",
    "    y_pred = model.predict(x).ravel()\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, \n",
    "             label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    return y_pred\n",
    "# Generate ROC curve on the test set\n",
    "pred = draw_roc_keras(model, x_test, y_test_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "451a3fa9-103a-4c4f-9cf8-ddd48ea8d19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/lstm_forda_data.npz with keys: x_train, x_val, x_test, y_train, y_val, y_test\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Save all required arrays to a single compressed .npz file\n",
    "np.savez_compressed(\n",
    "    \"data/lstm_forda_data.npz\",\n",
    "    x_train=X_train,\n",
    "    x_val=X_val,\n",
    "    x_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_val=y_val,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Saved: data/lstm_forda_data.npz with keys:\",\n",
    "    \", \".join([\"x_train\", \"x_val\", \"x_test\", \"y_train\", \"y_val\", \"y_test\"])\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lime_env)",
   "language": "python",
   "name": "lime_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
